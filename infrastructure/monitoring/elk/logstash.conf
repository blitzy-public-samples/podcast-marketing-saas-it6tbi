# Logstash Configuration v8.10.0
# Purpose: Process and transform log data from various sources before forwarding to Elasticsearch
# Requirement addressed: 7.4 Cross-Cutting Concerns/Monitoring and Observability - Log Ingestion and Transformation

# Input section - Defines the source of log data
input {
  file {
    # Monitor all log files in /var/log directory
    path => "/var/log/*.log"
    # Start reading from beginning of files
    start_position => "beginning"
    # Add file metadata
    add_field => {
      "[@metadata][service]" => "podcast-automation"
      "[@metadata][environment]" => "${ENVIRONMENT:production}"
    }
    # Prevent duplicate log entries
    sincedb_path => "/var/lib/logstash/sincedb"
    # Handle file rotation
    ignore_older => 86400 # 24 hours
    close_older => 3600 # 1 hour
  }
}

# Filter section - Transforms and enriches log data
filter {
  # Parse Apache common log format
  grok {
    match => { "message" => "%{COMMONAPACHELOG}" }
    # Add custom patterns for podcast-specific logs
    patterns_dir => ["/etc/logstash/patterns"]
    # Handle multiline stack traces
    pattern_definitions => {
      "STACKTRACE" => "^.+Exception.+$[\s\S]*?^\s*at .+$"
    }
  }

  # Add timestamps
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }

  # Enrich with service context
  mutate {
    add_field => {
      "service.name" => "podcast-automation"
      "service.version" => "${SERVICE_VERSION}"
    }
  }

  # Classify log levels
  if [message] =~ /ERROR|FATAL|SEVERE/ {
    mutate { add_field => { "log.level" => "error" } }
  } else if [message] =~ /WARN|WARNING/ {
    mutate { add_field => { "log.level" => "warning" } }
  } else if [message] =~ /INFO/ {
    mutate { add_field => { "log.level" => "info" } }
  } else if [message] =~ /DEBUG/ {
    mutate { add_field => { "log.level" => "debug" } }
  } else {
    mutate { add_field => { "log.level" => "unknown" } }
  }

  # Drop health check logs to reduce noise
  if [message] =~ /health[_-]check/ {
    drop { }
  }
}

# Output section - Defines where to send processed logs
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    # Dynamic index name based on date
    index => "logstash-%{+YYYY.MM.dd}"
    # Use custom template for podcast-specific mapping
    template_name => "podcast-logs"
    template_overwrite => true
    # Optimize bulk requests
    bulk_max_size => 5000
    # Handle backpressure
    flush_size => 1000
    idle_flush_time => 1
    # Retry configuration
    retry_initial_interval => 2
    retry_max_interval => 64
    retry_on_conflict => 3
    # SSL/TLS configuration
    ssl => true
    ssl_certificate_verification => true
    cacert => "/etc/logstash/certs/ca.crt"
    # Basic authentication
    user => "${ELASTICSEARCH_USER}"
    password => "${ELASTICSEARCH_PASSWORD}"
  }
}